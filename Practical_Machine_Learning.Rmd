---
title: "Practical Machine Learning Course Project"
author: "M. Hairul Othman"
date: "Sunday, January 31, 2016"
---

##Introduction

Data from accelerometers on the belt, forearm, arm, and dumbell of 6 participant will be used in this analysis. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The five ways are exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Only Class A corresponds to correct performance. The goal of this project is to predict the manner in which they did the exercise, i.e., Class A to E. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

##1. Data Processing

>Load the data sets
```{r}
training<-read.csv("data/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
testing<-read.csv('data/pml-testing.csv', na.strings=c("NA","#DIV/0!", ""))
```

>Install and load packages as follow:
```{r}
library(caret)
library(rattle)
library(gridExtra)
library(rpart)
library(randomForest)
```

>Setting the overall seed for reproduceability

```{r}
set.seed(1234)
```

##2. Data Cleaning

>Step 1: Remove missing values:
```{r}
training <- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]
```

>Step 2: Removes the inital seven columns of dimensional data:
```{r}
trainData <- training[, -c(1:7)]
testData <- testing[, -c(1:7)]
```

>Step 3: Trim dataset
This method will be used to increase data accuracy and minimize sample error. Partitioning the training data set to allow cross-validation. The training data set contains 53 variables and 19622 obs. The testing data set contains 53 variables and 20 obs. In order to perform cross-validation, the training data set is partionned into 2 sets: subTraining (75%) and subTesting (25%). This will be performed using random subsampling without replacement. Samples data were sliced by column against the feature set to fit the final model.

Partitioning the training data set to allow cross-validation.

```{r}
inTrain <- createDataPartition(y=trainData$classe, p=0.75, list=FALSE)
subTrain <- trainData[inTrain, ]
subTest <- trainData[-inTrain, ]
dim(subTrain)
dim(subTest)
```

##3. Prediction Algorithms
To predict the outcome, Decision Tree model and Random Forests model will be used to predict the outcome variable classe for the testing set.

>Desicion Tree
```{r}
DT_Model <- rpart(classe ~ ., data=subTrain, method="class")
DT_Predict <- predict(DT_Model, subTest, type="class")
```

Test results on our subTesting data set:
```{r}
confusionMatrix(DT_Predict, subTest$classe)
````

>Random Forest Model
*Fit model with random forests algorithm and 10-fold cross validation to predict classe with all other predictors.
*Plot accuracy of the model on the same scale as boosting model.

```{r}
RF_Model <- randomForest(classe ~. , data=subTrain, method="class")
RF_Predict <- predict(RF_Model, subTest, type = "class")
confusionMatrix(RF_Predict, subTest$classe)
```

>Find the accuracy rate:
```{r}
#(accuracy_rf <- conf_rf$overall[1])
predictFinal <- predict(RF_Model, testData)
predictFinal
```

###4. Conclusion
*Comparing both models generated, Random Forests model shows better accuracy with rate 0.995 compared to Decission Tree model was 0.739.
*The final Random Forests model contains 500 trees with 40 variables tried at each split. 
*Estimated out of sample error rate for the Random Forests model is 0.04% as reported by the final model.
