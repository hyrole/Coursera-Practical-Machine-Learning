---
title: "Practical Machine Learning Course Project"
author: "M. Hairul Othman"
date: "Sunday, January 31, 2016"
---

##Introduction

Data from accelerometers on the belt, forearm, arm, and dumbell of 6 participant will be used in this analysis. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The five ways are exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Only Class A corresponds to correct performance. The goal of this project is to predict the manner in which they did the exercise, i.e., Class A to E. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

##1. Data Processing

>Load the data sets
```{r}
training<-read.table("data/pml-training.csv", header=TRUE, sep=",")
testing<-read.table("data/pml-testing.csv",header=TRUE, sep=",")
```

>Install and load packages as follow:
```{r}
library(caret)
library(rattle)
library(gridExtra)
```

##2. Data Cleaning

>Step 1: Remove missing values:
```{r}
training <- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]
```

>Step 2: Removes the inital seven columns of dimensional data:
```{r}
trainData <- training[, -c(1:7)]
testData <- testing[, -c(1:7)]
```

>Step 3: Splitting data
This method will be used to increase data accuracy and minimize sample error. The full testing data is split randomly with a set seed with 80% of the data into the training sample and 20% of the data used as cross-validation. Samples data were sliced by column against the feature set to fit the final model.

```{r}
set.seed(7826)
inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)
train <- trainData[inTrain, ]
valid <- trainData[-inTrain, ]
```

##3. Prediction Algorithms
To predict the outcome, random forests model will be used.

```{r}
#install.packages('caret', dependencies = TRUE)
library(caret)
library(rpart)
library(randomForest)

control <- trainControl(method = "cv", number = 5)
```


##4. Prediction on Testing Set
>Boosting Model
*Fit model with boosting algorithm and 10-fold cross validation to predict classe with all other predictors.
*Plot accuracy of this model on the scale [0.9, 1].

```{r}
set.seed(123)
boostFit <- train(classe ~ ., method = "gbm", data = training, verbose = F, trControl = trainControl(method = "cv", number = 10))

#Plot boosting
boostFit
plot(boostFit, ylim = c(0.9, 1))
```
The boosting algorithm generated a good model with accuracy = 0.997.

>Random Forest Model
*In this section, random forests to predict the outcome variable classe for the testing set.
*Fit model with random forests algorithm and 10-fold cross validation to predict classe with all other predictors.
*Plot accuracy of the model on the same scale as boosting model.
```{r}
fit_rf <- train(classe ~ ., data = train, method = "rf", trControl = control)
print(fit_rf, digits = 4)

fit_rf <- train(classe ~ ., data = train, method = "rf", trControl = control)
print(fit_rf, digits = 4)

# predict outcomes using validation set
fit_rf <- predict(fit_rf, valid)
# Show prediction result
(fit_rf <- confusionMatrix(valid$classe, fit_rf))

(predict(fit_rf, testData))
```

>Find the accuracy rate:
```{r}
(accuracy_rf <- conf_rf$overall[1])
```


The random forests algorithm generated a very accurate model with accuracy close to 1. Compared to boosting model, this model generally has better performance in terms of accuracy as we see from the plots.

###5. Conclusion
*Comparing model accuracy of the two models generated, random forests and boosting, random forests model has overall better accuracy.
*The final random forests model contains 500 trees with 40 variables tried at each split. The five most important predictors in this model are r rownames(imp)[1:5].
*Estimated out of sample error rate for the random forests model is 0.04% as reported by the final model.
*Predict the test set and output results for automatic grader.